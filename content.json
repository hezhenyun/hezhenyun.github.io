[{"title":"爬虫中储存到文件夹的方法","date":"2019-12-04T13:57:57.972Z","path":"2019/12/04/os模块/","text":"1.创建文件夹12345filetitle = os.path.join(os.curdir, 'reads')建立文件夹，os.curdir(为当地地址)，创建语法为os.path.jion, 'reads'为文件夹的名称if not os.path.isdir(fileread): os.mkdir(fileread)os.path.isdir用来判断此文件夹是否存在，不存在则重新建立，建立方法为os.mkdir() 2.文件夹中文件的名称12filename = i + '.txt' # 每个文件的名字filename为传递的变量，i 为所选择的文件名，后缀为.txt 3.把文件存放在文件夹中1filepath = os.path.join(filetitle, filename) # 把每个文件都放在filetitle文件夹里 此处使用路径连接 ()里为文件夹变量以及文件名变量 最后写入并进行保存就OK了","tags":[]},{"title":"python strip()方法","date":"2019-12-02T04:52:30.127Z","path":"2019/12/02/strip方法/","text":"描述Python strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。 注意该方法只能删除开头或是结尾的字符，不能删除中间部分的字符。 语法strip()方法语法： 1str.strip([chars]); 参数chars – 移除字符串头尾指定的字符序列。 返回值返回移除字符串头尾指定的字符生成的新字符串。 实例以下实例展示了strip()函数的使用方法： 123456str = \"00000003210Runoob01230000000\"; print str.strip( '0' ); # 去除首尾字符 0 str2 = \" Runoob \"; # 去除首尾空格print str2.strip(); 以上实例输出结果如下： 123210Runoob0123Runoob 从结果上看，可以注意到中间部分的字符并未删除。 以上下例演示了只要头尾包含有指定字符序列中的字符就删除： 12str = \"123abcrunoob321\"print (str.strip( '12' )) # 字符序列为 12 以上实例输出结果如下： 13abcrunoob3","tags":[]},{"title":"Beautiful Soup基础","date":"2019-11-30T12:52:01.611Z","path":"2019/11/30/bs4/","text":"Beautiful Soup 是一个可以从HTML或XML文件中提取数据的Python库.它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式.Beautiful Soup会帮你节省数小时甚至数天的工作时间. 下面下面的一段HTML代码将作为例子被多次用到。 12345678910111213html_doc = \"\"\"&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=\"title\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;Elsie&lt;/a&gt;,&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=\"story\"&gt;...&lt;/p&gt;\"\"\" 几个简单的浏览结构化数据的方法: 12345678910111213141516171819202122232425262728soup.title# &lt;title&gt;The Dormouse's story&lt;/title&gt;soup.title.name# u'title'soup.title.string# u'The Dormouse's story'soup.title.parent.name# u'head'soup.p# &lt;p class=\"title\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;soup.p['class']# u'title'soup.a# &lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;soup.find_all('a')# [&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;,# &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;,# &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;]soup.find(id=\"link3\")# &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt; 从文档中找到所有标签的链接: 12345for link in soup.find_all('a'): print(link.get('href')) # http://example.com/elsie # http://example.com/lacie # http://example.com/tillie 从文档中获取所有文字内容: 123456789101112print(soup.get_text())# The Dormouse's story## The Dormouse's story## Once upon a time there were three little sisters; and their names were# Elsie,# Lacie and# Tillie;# and they lived at the bottom of a well.## ... 这是你想要的吗?别着急,还有更好用的 使用lxml解析器如何使用将一段文档传入BeautifulSoup 的构造方法,就能得到一个文档的对象, 可以传入一段字符串或一个文件句柄. 12345from bs4 import BeautifulSoupsoup = BeautifulSoup(open(\"index.html\"))soup = BeautifulSoup(\"&lt;html&gt;data&lt;/html&gt;\") 首先,文档被转换成Unicode,并且HTML的实例都被转换成Unicode编码 12BeautifulSoup(\"Sacr&amp;eacute; bleu!\")&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;Sacré bleu!&lt;/body&gt;&lt;/html&gt; 然后,Beautiful Soup选择最合适的解析器来解析这段文档,如果手动指定解析器那么Beautiful Soup会选择指定的解析器来解析文档.(参考 解析成XML ). 深度学习可以去Beautiful Soup官网去学习https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/","tags":[]},{"title":"xpath语法","date":"2019-11-28T13:27:23.310Z","path":"2019/11/28/xpath/","text":"XPath 使用路径表达式来选取 XML 文档中的节点或节点集。节点是通过沿着路径 (path) 或者步 (steps) 来选取的。 XML 实例文档我们将在下面的例子中使用这个 XML 文档。 123456789101112131415&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;bookstore&gt;&lt;book&gt; &lt;title lang=\"eng\"&gt;Harry Potter&lt;/title&gt; &lt;price&gt;29.99&lt;/price&gt;&lt;/book&gt;&lt;book&gt; &lt;title lang=\"eng\"&gt;Learning XML&lt;/title&gt; &lt;price&gt;39.95&lt;/price&gt;&lt;/book&gt;&lt;/bookstore&gt; 选取节点XPath 使用路径表达式在 XML 文档中选取节点。节点是通过沿着路径或者 step 来选取的。 下面列出了最有用的路径表达式： 表达式 描述123456nodename 选取此节点的所有子节点。/ 从根节点选取。// 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。. 选取当前节点。.. 选取当前节点的父节点。@ 选取属性。 在下面中，我们已列出了一些路径表达式以及表达式的结果： 路径表达式 结果123456789bookstore 选取 bookstore 元素的所有子节点。/bookstore 选取根元素 bookstore。 注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！bookstore/book 选取属于 bookstore 的子元素的所有 book 元素。//book 选取所有 book 子元素，而不管它们在文档中的位置。bookstore//book 选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置。//@lang 选取名为 lang 的所有属性。 谓语（Predicates）谓语用来查找某个特定的节点或者包含某个指定的值的节点。 谓语被嵌在方括号中。 在下面中，我们列出了带有谓语的一些路径表达式，以及表达式的结果： 路径表达式 结果12345678/bookstore/book[1] 选取属于 bookstore 子元素的第一个 book 元素。/bookstore/book[last()] 选取属于 bookstore 子元素的最后一个 book 元素。/bookstore/book[last()-1] 选取属于 bookstore 子元素的倒数第二个 book 元素。/bookstore/book[position()&lt;3] 选取最前面的两个属于 bookstore 元素的子元素的 book 元素。//title[@lang] 选取所有拥有名为 lang 的属性的 title 元素。//title[@lang='eng'] 选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。/bookstore/book[price&gt;35.00] 选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。/bookstore/book[price&gt;35.00]/title 选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。 选取未知节点XPath 通配符可用来选取未知的 XML 元素。 通配符 描述123* 匹配任何元素节点。@* 匹配任何属性节点。node() 匹配任何类型的节点。 在下面中，我们列出了一些路径表达式，以及这些表达式的结果： 路径表达式 结果123/bookstore/* 选取 bookstore 元素的所有子元素。//* 选取文档中的所有元素。//title[@*] 选取所有带有属性的 title 元素。 选取若干路径通过在路径表达式中使用”|”运算符，您可以选取若干个路径。 在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果： 路径表达式 结果123//book/title | //book/price 选取 book 元素的所有 title 和 price 元素。//title | //price 选取文档中的所有 title 和 price 元素。/bookstore/book/title | //price 选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。 参考菜鸟教程：https://www.runoob.com/xpath/xpath-syntax.html","tags":[]},{"title":"利用正则和爬虫知识爬取网站","date":"2019-11-26T06:01:39.069Z","path":"2019/11/26/正则爬虫框架/","text":"以下是自己在学习爬虫时，利用正则和爬虫知识爬取豆瓣电影排行榜的一些总结，仅供参考，如有不足，还请关照 1.请求在爬取网站是，首先就是先要发送自己的请求，看能不能访问到此网站，在这，我便以爬取豆瓣电影排行榜为例。在向豆瓣网站发送请求时，要添加一下自定义的headers,如下： 12345$ headers = &#123;$ 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/\\$ 537.36 (KHTML, like Gecko) Chrome/\\$ 78.0.3904.70 Safari/537.36'$ &#125; 在这里发送请求我用的是Requests,对于接触过爬虫的都会了解这个，我便不多说了 1$ response = requests.get(url, headers=headers) 这里的url指的就是所要爬取的网站的网址。 下面是一个完整的请求，用一个函数来概括。 1234567891011121314$ def get__one__page(url):$ try:$ headers = &#123;$ 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/\\$ 537.36 (KHTML, like Gecko) Chrome/\\$ 78.0.3904.70 Safari/537.36'$ &#125;$ response = requests.get(url, headers=headers)$ if response.status_code == 200:$ return response.text$ return None$ except RequestException:$ return None$ 在这里我用的是返回的响应码来做一个判断，当请求成功是返回的就是200，错误就会直接返回None,这里运用了一个异常判断来处理 2.解析在请求成功后，会返回对应网址的一些代码信息，这里就需要去解析这些代码信息，整理出来我们所需要的的数据信息 123456789101112131415$ def parse__one__page(html):$ pattern = re.compile($ '&lt;li&gt;.*?class=\"\"&gt;(\\d+)&lt;.*?src=\"(.*?)\".*?&gt;.*?title\"&gt;(.*?)&lt;.*?&gt;.*?&lt;p.*?&gt;(.*?)&lt;br&gt;.*?&lt;/p&gt;.*?average\"&gt;(.*?)&lt;.*?&lt;/li&gt;',$ re.S)$$ items = re.findall(pattern, html)$ # print(items)$ for item in items:$ yield &#123;$ 'index': item[0],$ 'image': item[1],$ 'title': item[2],$ 'director': item[3],$ 'score': item[4]$ &#125; 以上解析用的是正则表达式的知识来进行的，（如果对正则知识不熟练的可以参考我的另一篇文章），利用正则解析过，能够得到我们想要的信息后，会感觉还是比较乱，那么就要用到for循环迭代器了，进行有序的输出，使自己想要的数据信息更加清楚明白。 3.读入文件中在以上我们得到自己想要的信息后就要存放到本地的文件中，便于自己的浏览和使用，所以就要用到文件的知识了，这里就不多说了。 1234$ def write__one__page(content):$ with open('result.txt', 'a', encoding='utf-8') as f:$ f.write(json.dumps(content, ensure_ascii=False))$ f.write('\\n') 读入文件后，打开本地的文本文件，就可以显示了，好了就到这了。 主代码123456789101112131415$ def main(number):$ url = 'https://movie.douban.com/top250?start=' + number + '&amp;filter=' + str(number)$ html = get__one__page(url)$ # parse__one__page(html)$ # print(html)$ for item in parse__one__page(html):$ print(item)$ write__one__page(item)$$$ if __name__ == '__main__':$ for i in range(10):$ main(number=str(i * 25))$ time.sleep(1)$ # main() 阐述以上仅是自我的总结，仅供参考。","tags":[]},{"title":"Requests的基本用法","date":"2019-11-24T05:09:56.014Z","path":"2019/11/24/requests的用法/","text":"什么是RequestsRequests是Python语言编写，基于urllib3，采用Apache2 Licensed开源协议的HTTP库。它比urllib更加方便，可以节约我们大量的工作，完全满足HTTP测试需求。是Python实现的简单易用的HTTP库。 安装也很简单： pip3进行安装1$ pip3 install requests Request的语法操作1.实例引入123456789101112$ 1import requests$ response = requests.get( http://www.baidu.com/ )$ print(response.status_code)$ print(type(response.text))$ print(response.text)$ print(response.cookies)$$ 200$ &lt;class str &gt;$$ # ...HTML网页源码..$ &lt;RequestsCookieJar[]&gt;\" 2.各种请求方式123456$ import requests$$ requests.get( http://httpbin.org/get ) # 发送get请求$ requests.post( http://httpbin.org/post ) # 发送post请求，只要调用post方法，传入一个url参数$ requests.put( http://httpbin.org/put )$ requests.delete( http://httpbin.org/delete ) 请求1.基本GET请求1234$ import requests$ $ resp = requests.get( http://httpbin.org/get )$ print(resp.text) 2.带参数的GET请求这个我们前面有使用过，也是最常用的方法。运行成功就可以看到网页的源码了12345678$ import requests$$ data = &#123;$ name : jack ,$ age : 20$ &#125;$ resp = requests.get( http://httpbin.org/get , params=data)$ print(resp.text) 3.解析json传入参数只需要我们把数据生成一个字典，然后调用params参数，赋值给他就可以，是不是很方便。12345678$ import requests$ import json$$ resp = requests.get( http://httpbin.org/get )$ print(resp.text)$ print(resp.json())$ print(json.loads(resp.text))$ print(type(resp.json())) 可以看出Requests的jaon解析和json的loads方法解析出来的结果是完全一样的。所以Requests可以很方便的解析json数据。 4.获取二进制数据12345678$ import requests$$ resp = requests.get( http://www.baidu.com/img/baidu_jgylogo3.gif )$ print(resp.content)$ print(resp.text)$ $ with open( logo.gif , wb ) as f:$ f.write(resp.content) 运行成功我们可以看到content方法获取的图片页面源码是二进制数据，而text获取的则是字符串代码。显然获取图片这种二进制数据需要使用content方法。这样我们就保存了图片，我们可以在文件夹下看到这张图片。 5.添加headers12345$ import requests$$ headers = &#123; User-Agent : Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36 &#125;$ resp = requests.get( http://www.baidu.com , headers=headers)$ print(resp.text) 有些网页如果我们直接去请求的话，他会查看请求的对象是不是浏览器，如果没有浏览器信息就会禁止我们爬虫的访问，这个时候我们就要给爬虫加一个headers，加一个浏览器的user-agent信息。这样我们就可以正常访问了。如果有的伙伴不知道怎么得到User-Agent，可以打开浏览器的审查元素，找到network，随便点击一个链接就可以看到User-Agent的信息了。 6.基本POST请求12345678$ import requests$$ data = &#123;$ name : jack ,$ age : 20$ &#125;$ resp = requests.post( http://httpbin.org/post , data=data)$ print(resp.text) 一个POST必然是要有一个Form Data的表单提交的，我们只要把信息传给data参数就可以了。一个POST请求只需要调用post方法，是不是特别方便呢。如果不觉得方便的话，可以去参考urllib的使用方法。 响应1.response属性123456789$ import requests$$ response = requests.get( http://www.baidu.com/ )$ print(type(response.status_code)) # 状态码$ print(type(response.text)) # 网页源码$ print(type(response.headers)) # 头部信息$ print(type(response.cookies)) # Cookie$ print(type(response.url)) # 请求的url$ print(type(response.history)) # 访问的历史记录 2.状态码判断获取这些信息只需要简单的调用就可以实现了。1234567891011121314151617$ import requests$$ response = requests.get( http://www.baidu.com/ )$ exit() if not resp.status_code == 200 else print( Sucessful )$ Sucessful$ $ #如果发送了一个错误请求(一个4XX客户端错误，或者5XX服务器错误响应)，我们可以通过 Response.raise_for_status() 来抛出异常：$$ bad_r = requests.get( http://httpbin.org/status/404 )$ bad_r.status_code$ 404$ $ bad_r.raise_for_status()$ Traceback (most recent call last):$ File \"requests/models.py\", line 832, in raise_for_status$ raise http_error$ requests.exceptions.HTTPError: 404 Client Error 好了，这篇文章我们了解了Requests库的基本语法操作，相信大家对Requests库的请求和响应已经很清楚了，大家完全可以抓取一些网页了。 推荐阅读https://blog.csdn.net/qq_28168421/article/details/89530073","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://hezhenyun.github.io/tags/%E7%88%AC%E8%99%AB/"},{"name":"python","slug":"python","permalink":"http://hezhenyun.github.io/tags/python/"},{"name":"requests","slug":"requests","permalink":"http://hezhenyun.github.io/tags/requests/"}]},{"title":"Python2.x与3​​.x版本区别","date":"2019-11-22T04:37:57.548Z","path":"2019/11/22/python版本/","text":"Python的3​​.0版本，常被称为Python 3000，或简称Py3k。相对于Python的早期版本，这是一个较大的升级。 为了不带入过多的累赘，Python 3.0在设计的时候没有考虑向下相容。 许多针对早期Python版本设计的程式都无法在Python 3.0上正常执行。 为了照顾现有程式，Python 2.6作为一个过渡版本，基本使用了Python 2.x的语法和库，同时考虑了向Python 3.0的迁移，允许使用部分Python 3.0的语法与函数。 新的Python程式建议使用Python 3.0版本的语法。 除非执行环境无法安装Python 3.0或者程式本身使用了不支援Python 3.0的第三方库。目前不支援Python 3.0的第三方库有Twisted, py2exe, PIL等。 大多数第三方库都正在努力地相容Python 3.0版本。即使无法立即使用Python 3.0，也建议编写相容Python 3.0版本的程式，然后使用Python 2.6, Python 2.7来执行。 Python 3.0的变化主要在以下几个方面: print 函数print语句没有了，取而代之的是print()函数。 Python 2.6与Python 2.7部分地支持这种形式的print语法。在Python 2.6与Python 2.7里面，以下三种形式是等价的： 123$ print \"fish\"$ print (\"fish\") #注意print后面有个空格$ print(\"fish\") #print()不能带有任何其它参数 然而，Python 2.6实际已经支持新的print()语法： 12$ from __future__ import print_function$ print(\"fish\", \"panda\", sep=', ') 除法运算Python中的除法较其它语言显得非常高端，有套很复杂的规则。Python中的除法有两个运算符，/和// 首先来说/除法: 在python 2.x中/除法就跟我们熟悉的大多数语言，比如Java啊C啊差不多，整数相除的结果是一个整数，把小数部分完全忽略掉，浮点数除法会保留小数点的部分得到一个浮点数的结果。 在python 3.x中/除法不再这么做了，对于整数之间的相除，结果也会是浮点数。 Python 2.x:1234$ &gt;&gt;&gt; 1 / 2$ 0$ &gt;&gt;&gt; 1.0 / 2.0$ 0.5 Python 3.x:12$ &gt;&gt;&gt; 1/2$ 0.5 而对于//除法，这种除法叫做floor除法，会对除法的结果自动进行一个floor操作，在python 2.x和python 3.x中是一致的。 python 2.x:12$ &gt;&gt;&gt; -1 // 2$ -1 python 3.x:12$ &gt;&gt;&gt; -1 // 2$ -1 注意的是并不是舍弃小数部分，而是执行 floor 操作，如果要截取整数部分，那么需要使用 math 模块的 trunc 函数 python 3.x:12345$ &gt;&gt;&gt; import math$ &gt;&gt;&gt; math.trunc(1 / 2)$ 0$ &gt;&gt;&gt; math.trunc(-1 / 2)$ 0 异常在 Python 3 中处理异常也轻微的改变了，在 Python 3 中我们现在使用 as 作为关键词。 捕获异常的语法由 except exc, var 改为 except exc as var。 使用语法except (exc1, exc2) as var可以同时捕获多种类别的异常。 Python 2.6已经支持这两种语法。 12$ 1.在2.x时代，所有类型的对象都是可以被直接抛出的，在3.x时代，只有继承自BaseException的对象才可以被抛出。$ 2.2.xraise语句使用逗号将抛出对象类型和参数分开，3.x取消了这种奇葩的写法，直接调用构造函数抛出对象即可。 在2.x时代，异常在代码中除了表示程序错误，还经常做一些普通控制结构应该做的事情，在3.x中可以看出，设计者让异常变的更加专一，只有在错误发生的情况才能去用异常捕获语句来处理。 xrange在 Python 2 中 xrange() 创建迭代对象的用法是非常流行的。比如： for 循环或者是列表/集合/字典推导式。 这个表现十分像生成器（比如。”惰性求值”）。但是这个 xrange-iterable 是无穷的，意味着你可以无限遍历。 由于它的惰性求值，如果你不得仅仅不遍历它一次，xrange() 函数 比 range() 更快（比如 for 循环）。尽管如此，对比迭代一次，不建议你重复迭代多次，因为生成器每次都从头开始。 在 Python 3 中，range() 是像 xrange() 那样实现以至于一个专门的 xrange() 函数都不再存在（在 Python 3 中 xrange() 会抛出命名异常）。 12345678910import timeitn = 10000def test_range(n): return for i in range(n): passdef test_xrange(n): for i in xrange(n): pass Python 212345678910111213141516print 'Python', python_version()print '\\ntiming range()' %timeit test_range(n)print '\\n\\ntiming xrange()' %timeit test_xrange(n)Python 2.7.6timing range()1000 loops, best of 3: 433 µs per looptiming xrange()1000 loops, best of 3: 350 µs per loop Python 3123456789print('Python', python_version())print('\\ntiming range()')%timeit test_range(n)Python 3.4.1timing range()1000 loops, best of 3: 520 µs per loop 1234567print(xrange(10))---------------------------------------------------------------------------NameError Traceback (most recent call last)&lt;ipython-input-5-5d8f9b79ea70&gt; in &lt;module&gt;()----&gt; 1 print(xrange(10))NameError: name 'xrange' is not defined 参考菜鸟教程https://www.runoob.com/python/python-2x-3x.html","tags":[]},{"title":"python爬虫基本原理","date":"2019-11-20T13:40:50.584Z","path":"2019/11/20/爬虫基本原理/","text":"一、什么是爬虫互联网比喻成一张网，每张网上的节点就是数据存储的地方；Python程序类似蜘蛛，到每个节点中抓取自己的猎物；爬虫是指：给网站发送请求，获取资源后解析并提取有用数据的程序 二、爬虫的执行流程发送请求，获得数据（以图片，视频，js，css的形式），解析数据，存入数据库或文件中 爬虫执行流程1、发送请求request12345使用http库向目标站点发起请求，即发送一个RequestRequest包含：请求头、请求体等 Request模块缺陷：不能执行JS 和CSS 代码 2、获取响应内容 response123如果服务器能正常响应，则会得到一个ResponseResponse包含：html，json，图片，视频等 3、解析内容12345678910111213解析html数据：正则表达式（RE模块），第三方解析库如Beautifulsoup，pyquery等解析json数据：json模块解析二进制数据:以wb的方式写入文件请求头需要注意的参数：（1）Referrer：访问源至哪里来（一些大型网站，会通过Referrer 做防盗链策略；所有爬虫也要注意模拟）（2）User-Agent:访问的浏览器（要加上否则会被当成爬虫程序）（3）cookie：请求头注意携带 4、请求体1234567891011请求体如果是get方式，请求体没有内容（get请求的请求体放在 url后面参数中，直接能看到）如果是post方式，请求体是format data ps： 1、登录窗口，文件上传等，信息都会被附加到请求体内 2、登录，输入错误的用户名密码，然后提交，就可以看到post，正确登录后页面通常会跳转，无法捕捉到post 五、响应Response123456789101112131415161718192021222324252627282930311、响应状态码 200：代表成功 301：代表跳转 404：文件不存在 403：无权限访问 502：服务器错误2、respone header响应头需要注意的参数：（1）Set-Cookie:BDSVRTM=0; path=/：可能有多个，是来告诉浏览器，把cookie保存下来（2）Content-Location：服务端响应头中包含Location返回浏览器之后，浏览器就会重新访问另一个页面3、preview就是网页源代码JSO数据如网页html，图片二进制数据等 六、总结123456789101112131、总结爬虫流程： 爬取---&gt;解析---&gt;存储2、爬虫所需工具：请求库：requests,selenium（可以驱动浏览器解析渲染CSS和JS，但有性能劣势（有用没用的网页都会加载）；） 解析库：正则，beautifulsoup，pyquery 存储库：文件，MySQL，Mongodb，Redis 参考：作者：热心市民小蔡链接：https://www.jianshu.com/p/2411490d6464","tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://hezhenyun.github.io/tags/%E7%88%AC%E8%99%AB/"},{"name":"python","slug":"python","permalink":"http://hezhenyun.github.io/tags/python/"}]},{"title":"正则表达式之python系列","date":"2019-11-18T06:42:51.232Z","path":"2019/11/18/python正则/","text":"正则表达式的定义正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。10000 ~ 12数字组合09876536 长度 是否是0开头 正则表达式是对字符串（包括普通字符（例如，a 到 z 之间的字母）和特殊字符（称为“元字符”））操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。正则表达式是一种文本模式，模式描述在搜索文本时要匹配的一个或多个字符串。 正则表达式，又称正规表示式、正规表示法、正规表达式、规则表达式、常规表示法（英语：Regular Expression，在代码中常简写为regex、regexp或RE），是计算机科学的一个概念。正则表达式使用单个字符串来描述、匹配一系列匹配某个句法规则的字符串。在很多文本编辑器里，正则表达式通常被用来检索、替换那些匹配某个模式的文本。 Regular Expression的“Regular”一般被译为“正则”、“正规”、“常规”。此处的“Regular”即是“规则”、“规律”的意思，Regular Expression即“描述某种规则的表达式”之意。正则在所有语言中都有的内容。 正则表达式的作用和特点给定一个正则表达式和另一个字符串，我们可以达到如下的目的： 给定的字符串是否符合正则表达式的过滤逻辑（称作“匹配”）； 可以通过正则表达式，从字符串中获取我们想要的特定部分。 正则表达式的特点是： 灵活性、逻辑性和功能性非常强； 可以迅速地用极简单的方式达到字符串的复杂控制。 对于刚接触的人来说，比较晦涩难懂。 场景：如何判断一个字符串是手机号呢？ 判断邮箱为163或者126的所有邮件地址 假如你在写一个爬虫，你得到了一个网页的HTML源码。其中有一段 12345&lt;html&gt;&lt;body&gt;&lt;h1&gt;hello world&lt;h1&gt;&lt;/body&gt;&lt;/html&gt;你想要把这个hello world提取出来，但你这时如果只会python 的字符串处理，那么第一反应可能是$ s = \"&lt;html&gt;&lt;body&gt;&lt;h1&gt;hello world&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\"$ start_index = s.find('&lt;h1&gt;') python re模块：12345678910111213141516\\A：表示从字符串的开始处匹配\\Z：表示从字符串的结束处匹配，如果存在换行，只匹配到换行前的结束字符串。\\b：匹配一个单词边界，也就是指单词和空格间的位置。例如， 'py\\b' 可以匹配\"python\" 中的 'py'，但不能匹配 \"openpyxl\" 中的 'py'。\\B：匹配非单词边界。 'py\\b' 可以匹配\"openpyxl\" 中的 'py'，但不能匹配\"python\" 中的 'py'。\\d：匹配任意数字，等价于 [0-9]。 digit\\D：匹配任意非数字字符，等价于 [^\\d]。not digit\\s：匹配任意空白字符，等价于 [\\t\\n\\r\\f]。 space\\S：匹配任意非空白字符，等价于 [^\\s]。\\w：匹配任意字母数字及下划线，等价于[a-zA-Z0-9_]。\\W：匹配任意非字母数字及下划线，等价于[^\\w]\\\\：匹配原义的反斜杠\\。‘.’用于匹配除换行符（\\n）之外的所有字符。‘^’用于匹配字符串的开始，即行首。‘$’用于匹配字符串的末尾（末尾如果有换行符\\n，就匹配\\n前面的那个字符），即行尾。 定义正则验证次数：123456789101112131415‘*’用于将前面的模式匹配0次或多次（贪婪模式，即尽可能多的匹配） &gt;=0‘+’用于将前面的模式匹配1次或多次（贪婪模式） &gt;=1 ‘？’用于将前面的模式匹配0次或1次（贪婪模式） 0 ，1'&#123;m&#125;' 用于验证将前面的模式匹配m次'&#123;m,&#125;'用于验证将前面的模式匹配m次或者多次 &gt;=m'&#123;m,n&#125;' 用于验证将前面的模式匹配大于等于m次并且小于等于n次 ‘*？，+？，？？’即上面三种特殊字符的非贪婪模式（尽可能少的匹配）。 ‘&#123;m,n&#125;’用于将前面的模式匹配m次到n次（贪婪模式），即最小匹配m次，最大匹配n次。 ‘&#123;m,n&#125;？’即上面‘&#123;m,n&#125;’的非贪婪版本。‘\\\\’：'\\'是转义字符，在特殊字符前面加上\\，特殊字符就失去了其所代表的含义，比如\\+就仅仅代表加号+本身。‘[]’用于标示一组字符，如果^是第一个字符，则标示的是一个补集。比如[0-9]表示所有的数字，[^0-9]表示除了数字外的字符。‘|’比如A|B用于匹配A或B。‘(...)’用于匹配括号中的模式，可以在字符串中检索或匹配我们所需要的内容。 Python里数量词默认是贪婪的（在少数语言里也可能是默认非贪婪），总是尝试匹配尽可能多的字符； 非贪婪则相反，总是尝试匹配尽可能少的字符。 在”*”,”?”,”+”,”{m,n}”后面加上？，使贪婪变成非贪婪。 re.match函数re.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none。 函数语法：1$ re.match(pattern, string, flags=0) 函数参数说明：参数 描述 pattern 匹配的正则表达式string 要匹配的字符串。flags 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：正则表达式修饰符 - 可选标志匹配成功re.match方法返回一个匹配的对象，否则返回None。 我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。 匹配对象方法 描述 group(num=0) 匹配的整个表达式的字符串，group() 可以一次输入多个组号，在这种情况下它将返回一个包含那些组所对应值的元组。groups() 返回一个包含所有小组字符串的元组，从 1 到 所含的小组号。 re.search方法re.search 扫描整个字符串并返回第一个成功的匹配。 函数语法：1$ re.search(pattern, string, flags=0) 函数参数说明：参数 描述 pattern 匹配的正则表达式string 要匹配的字符串。flags 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。匹配成功re.search方法返回一个匹配的对象，否则返回None。 我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。 匹配对象方法 描述 group(num=0) 匹配的整个表达式的字符串，group() 可以一次输入多个组号，在这种情况下它将返回一个包含那些组所对应值的元组。groups() 返回一个包含所有小组字符串的元组，从 1 到 所含的小组号。 re.match与re.search的区别re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。 检索和替换Python 的 re 模块提供了re.sub用于替换字符串中的匹配项。 语法：1$ re.sub(pattern, repl, string, count=0, flags=0) 参数：1234pattern : 正则中的模式字符串。repl : 替换的字符串，也可为一个函数。string : 要被查找替换的原始字符串。count : 模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。 findall在字符串中找到正则表达式所匹配的所有子串，并返回一个列表，如果没有找到匹配的，则返回空列表。 注意： match 和 search 是匹配一次 findall 匹配所有。 语法格式为：1findall(string[, pos[, endpos]]) ###参数： 123string : 待匹配的字符串。pos : 可选参数，指定字符串的起始位置，默认为 0。endpos : 可选参数，指定字符串的结束位置，默认为字符串的长度。 re.splitsplit 方法按照能够匹配的子串将字符串分割后返回列表，它的使用形式如下： 1re.split(pattern, string[, maxsplit=0, flags=0]) 参数：参数 描述 pattern 匹配的正则表达式string 要匹配的字符串。maxsplit 分隔次数，maxsplit=1 分隔一次，默认为 0，不限制次数。flags 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：正则表达式修饰符 - 可选标志 参考本篇文章仅阐述正则表达式的一些理论知识，便于记忆若想要查看正则实例请于此地址浏览https://www.runoob.com/python/python-reg-expressions.html","tags":[{"name":"正则表达式","slug":"正则表达式","permalink":"http://hezhenyun.github.io/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"}]},{"title":"使用hexo+github搭建免费博客教程","date":"2019-11-16T07:52:31.118Z","path":"2019/11/16/hexo搭建教程/","text":"1.前言使用github pages服务搭建博客的好处有： 123456$ 全是静态文件，访问速度快；$ 免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台；$ 可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的；$ 数据绝对安全，基于github的版本管理，想恢复到哪个历史版本都行；$ 博客内容可以轻松打包、转移、发布到其它平台；$ 等等； 1.1.准备工作在开始一切之前，你必须已经： 123$ 有一个github账号，没有的话去注册一个；$ 安装了node.js、npm，并了解相关基础知识；$ 安装了git for windows（或者其它git客户端） 2.搭建github博客2.1.创建仓库新建一个名为”你的用户名.github.io”的仓库，比如说，如果你的github用户名是test，那么你就新建”test.github.io”的仓库（必须是你的用户名，其它名称无效），将来你的网站访问地址就是 http://test.github.io 了，是不是很方便？由此可见，每一个github账户最多只能创建一个这样可以直接使用域名访问的仓库。 几个注意的地方： 123$ 注册的邮箱一定要验证，否则不会成功；$ 仓库名字必须是：username.github.io，其中username是你的用户名；$ 仓库创建成功不会立即生效，需要过一段时间，大概10-30分钟，或者更久，我的等了半个小时才生效； 创建成功后，默认会在你这个仓库里生成一些示例页面，以后你的网站所有代码都是放在这个仓库里啦。 3.配置SSH Key为什么要配置这个呢？因为你提交代码肯定要拥有你的github权限才可以，但是直接使用用户名和密码太不安全了，所以我们使用ssh key来解决本地和服务器的连接问题。 1$ cd ~/. ssh #检查本机已存在的ssh密钥 如果提示：No such file or directory 说明你是第一次使用git。 1$ ssh-keygen -t rsa -C \"邮件地址\" 然后连续3次回车，最终会生成一个文件在用户目录下，打开用户目录，找到”.ssh\\id_rsa.pub”文件，记事本打开并复制里面的内容，打开你的github主页，进入个人设置 -&gt; SSH and GPG keys -&gt; New SSH key：将刚复制的内容粘贴到key那里，title随便填，保存。 3.1.测试是否成功1$ sh -T git@github.com # 注意邮箱地址不用改 如果提示Are you sure you want to continue connecting (yes/no)?，输入yes，然后会看到： 1Hi liuxianan! You've successfully authenticated, but GitHub does not provide shell access. 看到这个信息说明SSH已配置成功！此时你还需要配置： 12$ git config --global user.name \"liuxianan\"// 你的github用户名，非昵称$ git config --global user.email \"xxx@qq.com\"// 填写你的github注册邮箱 具体这个配置是干嘛的我没仔细深究。 4.hexo的使用和注意事项4.1.hexo简介Hexo是一个简单、快速、强大的基于 Github Pages 的博客发布工具，支持Markdown格式，有众多优秀插件和主题。 官网： http://hexo.iogithub: https://github.com/hexojs/hexo 4.2.原理由于github pages存放的都是静态文件，博客存放的不只是文章内容，还有文章列表、分类、标签、翻页等动态内容，假如每次写完一篇文章都要手动更新博文目录和相关链接信息，相信谁都会疯掉，所以hexo所做的就是将这些md文件都放在本地，每次写完文章后调用写好的命令来批量完成相关页面的生成，然后再将有改动的页面提交到github。 4.3.注意事项安装之前先来说几个注意事项： 1234$ 很多命令既可以用Windows的cmd来完成，也可以使用git $ bash来完成，但是部分命令会有一些问题，为避免不必要的问题，建议全部使用git bash来执行；$hexo不同版本差别比较大，网上很多文章的配置信息都是基于2.x的，所以注意不要被误导；hexo有2种\"_config.yml\"文件，一个是根目录下的全局的\"_config.yml\"，一个是各个theme下的； 4.4.安装1$ npm install -g hexo 4.5.初始化在电脑的某个地方新建一个名为hexo的文件夹（名字可以随便取），比如我的是”F:\\Workspaces\\hexo”，由于这个文件夹将来就作为你存放代码的地方，所以最好不要随便放。 1234$ cd /f/Workspaces/hexo/$ hexo init$ hexo g # 生成$ hexo s # 启动服务 hexo s是开启本地预览服务，打开浏览器访问 http://localhost:4000 即可看到内容，很多人会碰到浏览器一直在转圈但是就是加载不出来的问题，一般情况下是因为端口占用的缘故，因为4000这个端口太常见了 第一次初始化的时候hexo已经帮我们写了一篇名为 Hello World 的文章，默认的主题比较丑 4.6.上传之前在上传代码到github之前，一定要记得先把你以前所有代码下载下来（虽然github有版本管理，但备份一下总是好的），因为从hexo提交代码时会把你以前的所有代码都删掉。 4.7.上传到github如果你一切都配置好了，发布上传很容易，一句hexo d就搞定，当然关键还是你要把所有东西配置好。 首先，”ssh key”肯定要配置好。 其次，配置”_config.yml”中有关deploy的部分： 正确写法： 1234deploy: type: git repository: git@github.com:liuxianan/liuxianan.github.io.git branch: master 错误写法： 1234deploy: type: github repository: https://github.com/liuxianan/liuxianan.github.io.git branch: master 4.8.常用hexo命令 常见命令 1234567hexo new \"postName\" #新建文章hexo new page \"pageName\" #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，'ctrl + c'关闭server）hexo deploy #部署到GitHubhexo help # 查看帮助hexo version #查看Hexo的版本 缩写 1234hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy 组合命令 hexo s -g #生成并本地预览 hexo d -g #生成并上传 5.最终效果可以访问我的git博客来查看效果：https://hezhenyun.github.io/ 6.参考博客园：http://www.cnblogs.com/liuxianancopyright ©2012-2018 小茗同学","tags":[{"name":"hexo","slug":"hexo","permalink":"http://hezhenyun.github.io/tags/hexo/"},{"name":"github","slug":"github","permalink":"http://hezhenyun.github.io/tags/github/"}]},{"title":"Linux命令基础","date":"2019-11-16T07:52:26.859Z","path":"2019/11/16/Linux命令基础/","text":"引言1 熟悉在Linux操作系统下的基本操作，对Linux操作系统有一个感性认识2 参阅相关Linux的命令参考手册，熟悉Linux下的操作命令。 1.常用命令基础12345678910111213141516171819202122232425262728293031323334$ 登录系统：login 用户名称$ 注销（退出）系统：logout $ 关机命令：shutdown now$ 使用man命令帮助，例如：man ls（屏幕显示关于ls命令的帮助信息）$ ls（显示目录内容） 格式：ls [选项] [目录或是文件] 其中：-a 显示指定目录下所有子目录和文件，包括隐藏文件 -l 以长格式来显示文件的详细信息。 -R 递归地显示指定目录的各个子目录中的文件 例如： 使用ls 查看当前目录内容： $ ls 使用ls 查看指定目录内容： $ ls /etc 使用ls –al 查看当前目录内容： $ls -al 使用dir 查看当前目录内容： $dir$ cd （改变工作目录） 格式： cd [路径名称] 如：cd .. 回到上层目录 ；cd / 回到根目录$ pwd （显示当前工作目录的绝对路径） 格式： pwd $ mkdir （创建目录） 格式：mkdir [目录名称] 例如： mkdir /home/s2001/newdir $ rmdir （删除空目录） 格式：rmdir [选项] [目录名称]$ cp （文件或目录的复制） 格式：cp [选项] 源文件或目录 目标文件或目录 例如： cp 文件名1 文件名2$ mv （文件或目录更名或将文件由一个目录移到另一个目录中） 格式：mv [选项] 源文件或目录 目标文件或目录$ rm （删除文件或目录） 格式：rm [选项] 文件名|目录名$ cat （显示文件） 格式：cat [选项] 文件列表 例如：cat 文件名 $ cat命令也可用来建立新文件：cat &gt;文件名，ctrl+d结束输入 2.学习Linux辅助软件介绍虚拟机 Vmware，VirtualPC和Bochs等。Linux源代码阅读工具 Source Insight 源码阅读网站 http://lxr.linux.no","tags":[{"name":"Linux","slug":"Linux","permalink":"http://hezhenyun.github.io/tags/Linux/"}]},{"title":"Mysql命令基础大全","date":"2019-11-15T07:14:54.974Z","path":"2019/11/15/mysql命令/","text":"需知以下是整理的一些基础的Mysql的命令如果还需要深层的探索可以转接此地址Mysql官网 Mysql命令锦集1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 mysql -h主机地址 -u用户名 －p密码 连接MYSQL;如果刚安装好MYSQL，超级用户root是没有密码的。 （例：mysql -h110.110.110.110 -Uroot -P123456 注:u与root可以不用加空格，其它也一样） exit 退出MYSQL mysqladmin -u用户名 -p旧密码 password 新密码 修改密码 grant select on 数据库.* to 用户名@登录主机 identified by \"密码\"; 增加新用户。（注意：和上面不同，下面的因为是MYSQL环境中的命令，所以后面都带一个分号作为命令结束符） show databases; 显示数据库列表。刚开始时才两个数据库：mysql和test。mysql库很重要它里面有MYSQL的系统信息，我们改密码和新增用户，实际上就是用这个库进行操作。 use mysql； show tables; 显示库中的数据表 describe 表名; 显示数据表的结构 create database 库名; 建库 use 库名； create table 表名 (字段设定列表)； 建表 drop database 库名; drop table 表名； 删库和删表 delete from 表名; 将表中记录清空 select * from 表名; 显示表中的记录 mysqldump --opt school&gt;school.bbb 备份数据库：（命令在DOS的mysqlin目录下执行）;注释:将数据库school备份到school.bbb文件，school.bbb是一个文本文件，文件名任取，打开看看你会有新发现。 win2003系统下新增命令（实用部份）： shutdown /参数 关闭或重启本地或远程主机。 参数说明：/S 关闭主机，/R 重启主机， /T 数字 设定延时的时间，范围0～180秒之间， /A取消开机，/M //IP 指定的远程主机。 例：shutdown /r /t 0 立即重启本地主机（无延时） taskill /参数 进程名或进程的pid 终止一个或多个任务和进程。 参数说明：/PID 要终止进程的pid,可用tasklist命令获得各进程的pid，/IM 要终止的进程的进程名，/F 强制终止进程，/T 终止指定的进程及他所启动的子进程。 tasklist 显示当前运行在本地和远程主机上的进程、服务、服务各进程的进程标识符(PID)。 参数说明：/M 列出当前进程加载的dll文件，/SVC 显示出每个进程对应的服务，无参数时就只列出当前的进程。 Linux系统下基本命令 注：要区分大小写 uname 显示版本信息（同win2K的 ver） dir 显示当前目录文件,ls -al 显示包括隐藏文件（同win2K的 dir） pwd 查询当前所在的目录位置 cd cd ..回到上一层目录，注意cd 与..之间有空格。cd /返回到根目录。 cat 文件名 查看文件内容 cat &gt;abc.txt 往abc.txt文件中写上内容。 more 文件名 以一页一页的方式显示一个文本文件。 cp 复制文件 mv 移动文件 rm 文件名 删除文件，rm -a 目录名删除目录及子目录 mkdir 目录名 建立目录 rmdir 删除子目录，目录内没有文档。 chmod 设定档案或目录的存取权限 grep 在档案中查找字符串 diff 档案文件比较 find 档案搜寻 date 现在的日期、时间 who 查询目前和你使用同一台机器的人以及Login时间地点 w 查询目前上机者的详细资料 whoami 查看自己的帐号名称 groups 查看某人的Group passwd 更改密码 history 查看自己下过的命令 ps 显示进程状态 kill 停止某进程 gcc 黑客通常用它来编译C语言写的文件 su 权限转换为指定使用者 telnet IP telnet连接对方主机（同win2K），当出现bash$时就说明连接成功。 ftp ftp连接上某服务器（同win2K）","tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://hezhenyun.github.io/tags/Mysql/"},{"name":"命令","slug":"命令","permalink":"http://hezhenyun.github.io/tags/%E5%91%BD%E4%BB%A4/"}]},{"title":"Windows CMD命令大全","date":"2019-11-15T07:14:54.967Z","path":"2019/11/15/cmd命令大全/","text":"命令简介cmd是command的缩写.即命令行 。 虽然随着计算机产业的发展，Windows 操作系统的应用越来越广泛，DOS 面临着被淘汰的命运，但是因为它运行安全、稳定，有的用户还在使用，所以一般Windows 的各种版本都与其兼容，用户可以在Windows 系统下运行DOS，中文版Windows XP 中的命令提示符进一步提高了与DOS 下操作命令的兼容性，用户可以在命令提示符直接输入中文调用文件。 在9x系统下输入command就可以打开命令行.而在NT系统上可以输入cmd来打开，在windows2003后被cmd替代，利用CMD命令查询系统的信息或者是判断网络的好坏。 运行操作CMD命令：开始－&gt;运行－&gt;键入cmd或command(在命令行里可以看到系统版本、文件系统版本)CMD命令锦集 1. gpedit.msc-----组策略 2. sndrec32-------录音机 3. Nslookup-------IP地址侦测器 ，是一个 监测网络中 DNS 服务器是否能正确实现域名解析的命令行工具。 它在 Windows NT/2000/XP 中均可使用 , 但在 Windows 98 中却没有集成这一个工具。 4. explorer-------打开资源管理器 5. logoff---------注销命令 6. shutdown-------60秒倒计时关机命令 7. lusrmgr.msc----本机用户和组 8. services.msc---本地服务设置 9. oobe/msoobe /a----检查XP是否激活 10. notepad--------打开记事本 11. cleanmgr-------垃圾整理 12. net start messenger----开始信使服务 13. compmgmt.msc---计算机管理 14. net stop messenger-----停止信使服务 15. conf-----------启动netmeeting 16. dvdplay--------DVD播放器 17. charmap--------启动字符映射表 18. diskmgmt.msc---磁盘管理实用程序 19. calc-----------启动计算器 20. dfrg.msc-------磁盘碎片整理程序 21. chkdsk.exe-----Chkdsk磁盘检查 22. devmgmt.msc--- 设备管理器 23. regsvr32 /u *.dll----停止dll文件运行 24. drwtsn32------ 系统医生 25. rononce -p----15秒关机 26. dxdiag---------检查DirectX信息 27. regedt32-------注册表编辑器 28. Msconfig.exe---系统配置实用程序 29. rsop.msc-------组策略结果集 30. mem.exe--------显示内存使用情况 31. regedit.exe----注册表 32. winchat--------XP自带局域网聊天 33. progman--------程序管理器 34. winmsd---------系统信息 35. perfmon.msc----计算机性能监测程序 36. winver---------检查Windows版本 37. sfc /scannow-----扫描错误并复原 38. taskmgr-----任务管理器（2000/xp/2003 40. wmimgmt.msc----打开windows管理体系结构(WMI) 41. wupdmgr--------windows更新程序 42. wscript--------windows脚本宿主设置 43. write----------写字板 45. wiaacmgr-------扫描仪和照相机向导 46. winchat--------XP自带局域网聊天 49. mplayer2-------简易widnows media player 50. mspaint--------画图板 51. mstsc----------远程桌面连接 53. magnify--------放大镜实用程序 54. mmc------------打开控制台 55. mobsync--------同步命令 57. iexpress-------木马捆绑工具，系统自带 58. fsmgmt.msc-----共享文件夹管理器 59. utilman--------辅助工具管理器 61. dcomcnfg-------打开系统组件服务 62. ddeshare-------打开DDE共享设置 110. osk------------打开屏幕键盘 111. odbcad32-------ODBC数据源管理器 112. oobe/msoobe /a----检查XP是否激活 68. ntbackup-------系统备份和还原 69. narrator-------屏幕“讲述人” 70. ntmsmgr.msc----移动存储管理器 71. ntmsoprq.msc---移动存储管理员操作请求 72. netstat -an----(TC)命令检查接口 73. syncapp--------创建一个公文包 74. sysedit--------系统配置编辑器 75. sigverif-------文件签名验证程序 76. ciadv.msc------索引服务程序 77. shrpubw--------创建共享文件夹 78. secpol.msc-----本地安全策略 79. syskey---------系统加密，一旦加密就不能解开，保护windows xp系统的双重密码 80. services.msc---本地服务设置 81. Sndvol32-------音量控制程序 82. sfc.exe--------系统文件检查器 83. sfc /scannow---windows文件保护 84. ciadv.msc------索引服务程序 85. tourstart------xp简介（安装完成后出现的漫游xp程序） 86. taskmgr--------任务管理器 87. eventvwr-------事件查看器 88. eudcedit-------造字程序 89. compmgmt.msc---计算机管理 90. packager-------对象包装程序 91. perfmon.msc----计算机性能监测程序 92. charmap--------启动字符映射表 93. cliconfg-------SQL SERVER 客户端网络实用程序 94. Clipbrd--------剪贴板查看器 95. conf-----------启动netmeeting 96. certmgr.msc----证书管理实用程序 97. regsvr32 /u *.dll----停止dll文件运行 98. regsvr32 /u zipfldr.dll------取消ZIP支持 99. cmd.exe--------CMD命令提示符","tags":[{"name":"Windows","slug":"Windows","permalink":"http://hezhenyun.github.io/tags/Windows/"},{"name":"CMD","slug":"CMD","permalink":"http://hezhenyun.github.io/tags/CMD/"}]},{"title":"Hello World","date":"2019-11-15T02:57:43.525Z","path":"2019/11/15/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]},{"title":"你好,欢迎来到我的博客","date":"2019-10-14T06:48:00.000Z","path":"2019/10/14/你好-Hexo/","text":"","tags":[]}]